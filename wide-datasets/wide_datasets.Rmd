---
title: "Transforming Wide Datasets"
author: "Jeff Parks"
date: "`r Sys.Date()`"
description: "CUNY Data 607 - Fall 2021, Project 2"
output:
#  pdf_document:
#    latex_engine: xelatex
#    extra_dependencies: ["geometry", "multicol", "multirow"]
#  prettydoc::html_pretty:
#    theme: architect
#    highlight: github
  rmdformats::robobook:
    highlight: kate
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(knitr)
library(kableExtra)
```

## Assignment

Three different untidy (or "wide") datasets were selected in order to practice different methods of data preparation.

## Dataset 1: World Governmental Indicators

This dataset from the World Bank ([source](http://info.worldbank.org/governance/wgi/)) reports "aggregate and individual governance indicators for over 200 countries and territories over the period 1996â€“2020, for six dimensions of governance:"

- Voice and Accountability
- Political Stability and Absence of Violence/Terrorism
- Government Effectiveness
- Regulatory Quality
- Rule of Law
- Control of Corruption

The source file is an Excel workbook (.xlsx) with multiple tabs, and for this exercise we'll select one tab to focus on - **Control of Corruption**.  

There is good deal of header text on this tab, and a 'multiindex' of column data pairing years with six numeric measures (Estimate, StdErr, NumSrc, Rank, Lower and Upper) resulting in 134 total columns.

![Tab from wgidataset.xlsx](https://raw.githubusercontent.com/jefedigital/cuny-data-607-projects/main/wide-datasets/images/wgi.png)

### Data Prep

The **ReadXL** library had some trouble reading in an .xlsx from a remote URL, so we'll use **Curl** to download a local copy first.  Then we'll read in the data from the **ControlofCorruption** tab, skipping all twelve rows of header information.

```{r, message=FALSE, warning=FALSE}
library(curl)
library(readxl)

# import excel sheet and skip header rows
#curl_download('http://info.worldbank.org/governance/wgi/Home/downLoadFile?fileName=wgidataset.xlsx', 
#              'data/wgi/wgidataset.xlsx') # first run only

df_raw <- read_excel('data/wgi/wgidataset.xlsx', 
                     sheet='ControlofCorruption', skip=12)
```

```{r, echo=FALSE, message=FALSE, warning=FALSE}
kable(df_raw[1:5,1:4], caption="sample of df_raw") %>%
  kable_minimal()
```

We'll want to create a **pivot_longer** of all this column data, but first we need to handle this multi-index in order to get a single row for each Country, Year and Measure.

Unlike Python's **Pandas**, there is not much native support for multi-index dataframes in R.  One approach would be to grab the two rows containing Year and Measure, and fuse them into individual column names for a new dataframe that we can then pivot out:

```{r, message=FALSE, warning=FALSE}
# convert the column names and first row into 2 vectors
df_col_1 <- colnames(df_raw) # colnames() returns a vector
df_col_2 <- as.character(slice(df_raw,1)) # slice() returns a df, so make vector

# get rid of the auto numbering in the column names
df_col_1 <- str_remove(df_col_1, '\\..*$')

# now join each element of the two vectors with paste0 (no separators)
df_cols <- paste0(df_col_1, df_col_2)

# preview
df_cols[1:8]

# make new df with our new column names and the values
df <- df_raw[-1,]
names(df) <- df_cols
```

```{r, echo=FALSE, message=FALSE, warning=FALSE}
kable(df[1:5,1:5], caption="new df") %>%
  kable_minimal()
```

A bit of data cleanup before we go further.  There are numerous instances of the string **#N/A** throughout this dataset, but we'll need to replace those with actual **NA** values that R will recognize as NULL instead of character strings.   

One way to apply this across all columns at once is to define a function that uses **str_replace** and use **mutate(across(everything(), .func))** to execute.

(Note: We could have done this after the following step instead, when we only have one column to deal with, but it's good practice..)

```{r, message=FALSE, warning=FALSE}

# replace all the instances of string '#N/A' with actual NA
nona <- function(s) {str_replace(s,'#N/A','NA')}
df <- df %>% mutate(across(everything(), nona))

# convert all available columns to numeric type
df <- type_convert(df)
```

```{r, echo=FALSE, message=FALSE, warning=FALSE}
kable(df[1:5,1:5], caption="formatted df") %>%
  kable_minimal()
```

Now for the pivot!  

The **pivot_longer** function has a parameter **names_pattern** that accepts regex capturing groups to split column names during the pivot.

In this case we'll create two capture groups .. one for the four-digit year, and one for the Measure name:

```{r, message=FALSE, warning=FALSE}
# we can use the names_pattern parameter of pivot_longer to separate Year and Category into separate columns .. pretty useful!
df <- df %>% pivot_longer(
 cols = `1996Estimate`:`2020Upper`,
 names_to = c('Year','Measure'),
 names_pattern = '(.{4})(.*)',
 values_to = 'Value'
)
```

```{r, echo=FALSE, message=FALSE, warning=FALSE}
kable(df[50:55,], caption="pivoted df") %>%
  kable_minimal()
```

### Analysis

We now have a tidy, well-formatted datset with each row representing a single observation of a given Year, Country and Measure.  Let's demonstrate with a couple of simple graphs:

```{r, fig.show="hold", fig.align = "center"}
df_rank_2020 <- df %>% 
  filter(Measure=='Rank', Year==2020) %>% 
  filter(Value >= 90) %>%
  arrange(desc(Value))

df_rank_2020 %>% ggplot(aes(x=reorder(Code, -Value), y=Value)) +
  geom_bar(stat='identity', color = "#112446", fill="#ffffff") +
  theme_minimal() + 
  theme(axis.text.x = element_text(angle = 90)) +
  labs(title='Top Countries', x='Year')

df_rank_annual <- df %>% 
  filter(Measure=='Rank', Code=='AFG') %>% 
  arrange(Year)

df_rank_annual %>% ggplot(aes(x=Year, y=Value)) +
  geom_bar(stat='identity', color = "#112446", fill="#ffffff") +
  theme_minimal() + 
  theme(axis.text.x = element_text(angle = 90)) + 
  labs(title='Afghanistan')
```

---

## Dataset 2: League of Legends Champions

Video game developer **Riot Games** publishes **League of Legends** game data and assets for use by third-party developers.   We'll be looking at the top-level dataset ([source](https://developer.riotgames.com/docs/lol#data-dragon)) for the main game characters (or "Champions"), which is made available in JSON format.

![champions.json](https://raw.githubusercontent.com/jefedigital/cuny-data-607-projects/main/wide-datasets/images/lol.png)


### Data Prep

We'll be using two additional libraries to parse HTML and work efficiently with JSON-formatted data: **rvest** and **tidyjson**.
```{r, message=FALSE, warning=FALSE}
library(tidyjson)
library(rvest)

# get json from webpage with rvest read_html
page <- read_html('https://ddragon.leagueoflegends.com/cdn/11.19.1/data/en_US/champion.json')

# parse the html with rvest, return just the contents of the <body> element
json <- page %>% html_elements("body") %>% html_text()

# check out the structure
json %>% gather_object %>% json_types
```

The tricky part here is, if we were to use **tidyjson** at the root of this JSON document, we wouldn't get the information we need.  

What we actually need is the fourth node (helpfully named "data"), which itself is a series of nested JSON nodes that contain all the character information. 

We'll use the **enter_object** and **gather_object** functions to extract the content of this "data" node, **spread_all** to create a table of all the json elements, and then convert back into a standard R dataframe:

```{r, message=FALSE, warning=FALSE}
# get only the 'data' node from the json and spread_all into a 
# json table, then convert back to regular dataframe
champions <- json %>% 
  enter_object(data) %>% 
  gather_object %>% 
  spread_all %>% 
  as_data_frame.tbl_json %>%
  select(!blurb) # delete this column for demo .. very long strings
```

```{r, echo=FALSE, message=FALSE, warning=FALSE}
kable(head(champions), caption="formatted df") %>%
  kable_minimal()
```

### Analysis

**tidyjson** has expanded all the information from the "data" node into a well-formatted dataframe, and has automatically prepended any sub-node names such as "info" and stats" to make it easy to differentiate.

Let's examine the results with a couple of simple graphs:

```{r, fig.show="hold", fig.align = "center"}
df <- champions %>% 
  select(c('name','stats.hp','stats.mp','stats.movespeed','stats.armor'))

ggplot(df) +
  aes(x = stats.hp) +
  geom_histogram(binwidth = 10, color = "#112446", fill="#ffffff") +
  theme_minimal() + 
  labs(title='Hit Points')

ggplot(df) +
  aes(x = stats.mp) +
  geom_histogram(binwidth=50, color = "#112446", fill="#ffffff") +
  theme_minimal() +
  labs(title='Mana Points')
```

---

## Dataset 3: Global Infant Mortality Rates

This dataset published by the World Bank ([source](https://data.worldbank.org/indicator/SH.DYN.MORT)) tracks the mortality rate of children under 5, measured as deaths per 1,000 live births.   The data are broken down by country, region and year from 1960 to 2019.

### Data Prep

This dataset includes three separate files, one with the Mortality data by country and year, and two files with additional metadata.  

For this exercise we will load the Mortality data file and the Region metadata file, then transform and join the two together for well-formatted and informative dataset.
```{r, message=FALSE, warning=FALSE}
df_mort <- read_csv('https://raw.githubusercontent.com/jefedigital/cuny-data-607-projects/main/wide-datasets/data/worldbank/API_SH.DYN.MORT_DS2_en_csv_v2_3012069.csv', skip=4)
df_meta <- read_csv('https://raw.githubusercontent.com/jefedigital/cuny-data-607-projects/main/wide-datasets/data/worldbank/Metadata_Country_API_SH.DYN.MORT_DS2_en_csv_v2_3012069.csv')

# cleanup
df_mort <- df_mort %>% 
  select(!('2020':'...66')) %>% # drop the last 2 columns, no data
  select(!('Indicator Name':'Indicator Code')) %>% # drop these cols, not needed
  rename(Name = 'Country Name', Code = 'Country Code') # renaming

df_meta <- df_meta %>%
  select(!SpecialNotes) %>% # drop column 'SpecialNotes', not needed
  select(!last_col()) # drop last column, no data
```

One additional step on the regional metadata .. we will use the conditional statement **ifelse** to impute whether the observation pertains to a Country or a Region, and to impute the value from the TableName column where Region is NA.

(This is due to a very large number of 'secondary' Region categories, and the likelihood of wanting to look at Country-only and Region-only data in any further analysis.)

```{r, message=FALSE, warning=FALSE}
# impute missing Region values
df_meta <- df_meta %>% 
  mutate(`Record Type` = ifelse(is.na(Region),'Region','Country')) %>%
  mutate(Region = ifelse(is.na(Region), TableName, Region)) %>% 
  rename(Code = 'Country Code') %>%
  select(!TableName)
```

Now, we pivot the Mortality table with **pivot_longer** so that each row corresponds to a single Country and Year.   We then **left_join** the Region metadata and perform a little cleanup.
```{r, message=FALSE, warning=FALSE}
# pivot
df_mort <- df_mort %>% 
  pivot_longer(!c('Name','Code'), 
               names_to = 'Year', 
               values_to = 'Mortality Rate')

# join
df <- left_join(df_mort, df_meta, by='Code')

# correct Mortality Rate to actual rate (/1000)
df <- df %>% mutate(`Mortality Rate` = `Mortality Rate`/1000)
```

```{r, echo=FALSE, message=FALSE, warning=FALSE}
kable(df[70:75,1:5], caption="final df (sample)") %>%
  kable_minimal()
```

To aid in analysis, we'll filter two subtables - one with Country data only, and one with Region data only.  

Let's also take a look at the unique names in each table to compare the "main" geo-based Regions in the Countries table, with a much larger number of "secondary" Regions in the Region table:

```{r, message=FALSE, warning=FALSE}
# analysis
df_regions <- df %>% filter(`Record Type` == 'Region')
df_countries <- df %>% filter(`Record Type` == 'Country')

# unique regions in the country table 
unique(df_countries$Region)

# unique regions in the regions table
unique(df_regions$Name)
```

### Analysis

We should have a good foundation now for further analysis.  

Let's demonstrate with a line graph of Mortality Rates by country, from the Latin America and Caribbean region.  Right away, we can see that while child mortality has been steadily dropping, there are a few alarming outliers where the trend has reversed in recent years, or where catastrophic events have had a significant impact (such as the 2010 Haiti earthquake, seen in the top line below.)

```{r, message=FALSE, warning=FALSE}
df_countries_latam <- df_countries %>%
  filter(Region == 'Latin America & Caribbean')

ggplot(df_countries_latam) +
  aes(x = Year, y = `Mortality Rate`, group = Name, label=Name) +
  geom_line() +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 90))
```